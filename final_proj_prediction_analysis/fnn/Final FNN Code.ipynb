{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de10229f-89fe-42e6-a442-7f2d1ae7aa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eae0abe-8aa4-48e2-900f-e315a33a588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "data = pd.read_csv('final_gentrification_dataset_with_labels.csv')\n",
    "\n",
    "# get features ready \n",
    "change_features = [\n",
    "    'income_diversity', 'racial_diversity', 'bachelor_or_higher', 'crowding_rate',\n",
    "    'pop_above_65', 'under_18', 'homeownership', 'born_in_ny_rate'\n",
    "]\n",
    "for feature in change_features:\n",
    "    data[f'{feature}_change'] = data[f'{feature}_2019'] - data[f'{feature}_2017']\n",
    "\n",
    "\n",
    "features = [f'{feature}_change' for feature in change_features] + [\n",
    "    'ela_grade4_2017', 'math_grade4_2017', 'foreclosure_rate_2017', 'park_access_2017'\n",
    "]\n",
    "X = data[features].values\n",
    "y = data['gentrification_label'].values\n",
    "\n",
    "# one hot encode y feature\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "y_one_hot = np.zeros((len(y), 3))\n",
    "y_one_hot[np.arange(len(y)), y_encoded] = 1\n",
    "\n",
    "\n",
    "# scale x features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_one_hot, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "test_indices = data.index[-len(X_test):] if not hasattr(X_test, 'index') else X_test.index\n",
    "test_data = data.iloc[test_indices].copy()\n",
    "test_data['actual_label'] = le.inverse_transform(np.argmax(y_test, axis=1))\n",
    "\n",
    "\n",
    "class_weights = np.array([1.5, 1.0, 1.5])\n",
    "\n",
    "\n",
    "d = X_train.shape[1] \n",
    "h1 = 16  # number of neurons in 1st layer\n",
    "h2 = 8   # number of neurons in 2nd layer\n",
    "o = 3 \n",
    "\n",
    "W1 = np.random.randn(d, h1) * 0.01 \n",
    "b1 = np.zeros((h1, 1))             \n",
    "W2 = np.random.randn(h1, h2) * 0.01 \n",
    "b2 = np.zeros((h2, 1))              \n",
    "W3 = np.random.randn(h2, o) * 0.01  \n",
    "b3 = np.zeros((o, 1))               \n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_deriv(x):\n",
    "    return np.heaviside(x, 0)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "\n",
    "def compute_loss(y_true, y_pred, class_weights):\n",
    "    y_true = y_true.reshape(-1, 3)\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    loss = -np.sum(y_true * np.log(y_pred) * class_weights) / y_true.shape[0]\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa043db-3faf-4279-9ad1-6dc91c8a93fd",
   "metadata": {},
   "source": [
    "# Experimenting with Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e0cc461-a414-4194-9378-cf0cb7f14036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " eta = 0.0001\n",
      "Learning Rate: 0.0001\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Gentrifying       0.00      0.00      0.00         3\n",
      "  Higher-Income       0.55      1.00      0.71         6\n",
      "Non-Gentrifying       0.00      0.00      0.00         2\n",
      "\n",
      "       accuracy                           0.55        11\n",
      "      macro avg       0.18      0.33      0.24        11\n",
      "   weighted avg       0.30      0.55      0.39        11\n",
      "\n",
      "\n",
      " eta = 0.001\n",
      "Learning Rate: 0.001\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Gentrifying       0.00      0.00      0.00         3\n",
      "  Higher-Income       0.55      1.00      0.71         6\n",
      "Non-Gentrifying       0.00      0.00      0.00         2\n",
      "\n",
      "       accuracy                           0.55        11\n",
      "      macro avg       0.18      0.33      0.24        11\n",
      "   weighted avg       0.30      0.55      0.39        11\n",
      "\n",
      "\n",
      " eta = 0.005\n",
      "Learning Rate: 0.005\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Gentrifying       0.00      0.00      0.00         3\n",
      "  Higher-Income       0.55      1.00      0.71         6\n",
      "Non-Gentrifying       0.00      0.00      0.00         2\n",
      "\n",
      "       accuracy                           0.55        11\n",
      "      macro avg       0.18      0.33      0.24        11\n",
      "   weighted avg       0.30      0.55      0.39        11\n",
      "\n",
      "\n",
      " eta = 0.01\n",
      "Learning Rate: 0.01\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Gentrifying       0.00      0.00      0.00         3\n",
      "  Higher-Income       0.55      1.00      0.71         6\n",
      "Non-Gentrifying       0.00      0.00      0.00         2\n",
      "\n",
      "       accuracy                           0.55        11\n",
      "      macro avg       0.18      0.33      0.24        11\n",
      "   weighted avg       0.30      0.55      0.39        11\n",
      "\n",
      "\n",
      "Summary of eta Experiment \n",
      "Learning Rate: 0.0001, Test Accuracy: 0.55\n",
      "Learning Rate: 0.001, Test Accuracy: 0.55\n",
      "Learning Rate: 0.005, Test Accuracy: 0.55\n",
      "Learning Rate: 0.01, Test Accuracy: 0.55\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_and_train_model(eta):\n",
    "    np.random.seed(42)\n",
    "    n = X_train.shape[0] \n",
    "    d = X_train.shape[1] \n",
    "    h1 = 16 \n",
    "    h2 = 8\n",
    "    o = 3\n",
    "    \n",
    "    W1 = np.random.randn(d, h1) * 0.01\n",
    "    b1 = np.zeros((h1, 1))\n",
    "    W2 = np.random.randn(h1, h2) * 0.01\n",
    "    b2 = np.zeros((h2, 1))\n",
    "    W3 = np.random.randn(h2, o) * 0.01\n",
    "    b3 = np.zeros((o, 1))\n",
    "    \n",
    "    epochs = 400\n",
    "    n = X_train.shape[0]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        dW1, db1 = np.zeros_like(W1), np.zeros_like(b1)\n",
    "        dW2, db2 = np.zeros_like(W2), np.zeros_like(b2)\n",
    "        dW3, db3 = np.zeros_like(W3), np.zeros_like(b3)\n",
    "        \n",
    "        for i in range(n):\n",
    "            x = X_train[i].reshape(d, 1)\n",
    "            y_true = y_train[i].reshape(o, 1)\n",
    "                 \n",
    "            z1 = np.dot(W1.T, x) + b1\n",
    "            h1 = relu(z1)\n",
    "            z2 = np.dot(W2.T, h1) + b2\n",
    "            h2 = relu(z2)\n",
    "            z3 = np.dot(W3.T, h2) + b3\n",
    "            y_pred = softmax(z3)\n",
    "            y_pred, h1, h2, z1, z2\n",
    "            \n",
    "            loss += compute_loss(y_true, y_pred, class_weights)\n",
    "            \n",
    "            dz3 = y_pred - y_true\n",
    "            dW3 += np.dot(h2,dz3.T)\n",
    "            db3 += dz3\n",
    "            \n",
    "            dh2 = np.dot(W3, dz3)\n",
    "            dz2 = dh2 * relu_deriv(z2)\n",
    "            dW2 += np.dot(h1,dz2.T)\n",
    "            db2 += dz2\n",
    "            \n",
    "            dh1 = np.dot(W2, dz2)\n",
    "            dz1 = dh1 * relu_deriv(z1)\n",
    "            dW1 += np.dot(x,dz1.T)\n",
    "            db1 += dz1\n",
    "        \n",
    "        dW1 /= n\n",
    "        db1 /= n\n",
    "        dW2 /= n\n",
    "        db2 /= n\n",
    "        dW3 /= n\n",
    "        db3 /= n\n",
    "        \n",
    "        W1 -= eta * dW1\n",
    "        b1 -= eta * db1\n",
    "        W2 -= eta * dW2\n",
    "        b2 -= eta * db2\n",
    "        W3 -= eta * dW3\n",
    "        b3 -= eta * db3\n",
    "        \n",
    "    \n",
    "    y_pred_test = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        x = X_test[i].reshape(d, 1)\n",
    "        z1 = np.dot(W1.T, x) + b1\n",
    "        h1 = relu(z1)\n",
    "        z2 = np.dot(W2.T, h1) + b2\n",
    "        h2 = relu(z2)\n",
    "        z3 = np.dot(W3.T, h2) + b3\n",
    "        y_pred = softmax(z3)\n",
    "        y_pred_test.append(np.argmax(y_pred))\n",
    "    y_pred_test = np.array(y_pred_test)\n",
    "    y_test_labels = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test_labels, y_pred_test)\n",
    "    conf_matrix = confusion_matrix(y_test_labels, y_pred_test)\n",
    "    class_report = classification_report(y_test_labels, y_pred_test, labels=[0, 1, 2], target_names=le.classes_, zero_division=0)\n",
    "    \n",
    "    test_data['predicted_label'] = le.inverse_transform(y_pred_test)\n",
    "    \n",
    "    print(f\"Learning Rate: {eta}\")\n",
    "    print(\"Classification Report:\\n\", class_report)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.005, 0.01]\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "for eta in learning_rates:\n",
    "    print(f\"\\n eta = {eta}\")\n",
    "    accuracy = build_and_train_model(eta)\n",
    "    results.append((eta, accuracy))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nSummary of eta Experiment \")\n",
    "for eta, acc in results:\n",
    "    print(f\"Learning Rate: {eta}, Test Accuracy: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f17ea46-0be0-4949-8f06-fdc8be7cbeb0",
   "metadata": {},
   "source": [
    "# Experimenting with Dropout Rate and adding Batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5d80228-dc38-4a2b-8449-596a0dd82ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reusable function for dropout\n",
    "def apply_dropout(h, p):\n",
    "    if p > 0:\n",
    "        mask = np.random.binomial(1, 1-p, size=h.shape)\n",
    "        h_dropped = h * mask / (1-p)\n",
    "        return h_dropped, mask\n",
    "    return h, np.ones_like(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d31980c7-9c57-44ae-bd13-3e1dc8217270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dropout rate = 0.0\n",
      "\n",
      "Dropout Rate: 0.0\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Gentrifying       1.00      0.67      0.80         3\n",
      "  Higher-Income       0.83      0.83      0.83         6\n",
      "Non-Gentrifying       0.33      0.50      0.40         2\n",
      "\n",
      "       accuracy                           0.73        11\n",
      "      macro avg       0.72      0.67      0.68        11\n",
      "   weighted avg       0.79      0.73      0.75        11\n",
      "\n",
      "\n",
      "dropout rate = 0.1\n",
      "\n",
      "Dropout Rate: 0.1\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Gentrifying       0.67      0.67      0.67         3\n",
      "  Higher-Income       0.80      0.67      0.73         6\n",
      "Non-Gentrifying       0.33      0.50      0.40         2\n",
      "\n",
      "       accuracy                           0.64        11\n",
      "      macro avg       0.60      0.61      0.60        11\n",
      "   weighted avg       0.68      0.64      0.65        11\n",
      "\n",
      "\n",
      "dropout rate = 0.3\n",
      "\n",
      "Dropout Rate: 0.3\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Gentrifying       1.00      0.67      0.80         3\n",
      "  Higher-Income       0.83      0.83      0.83         6\n",
      "Non-Gentrifying       0.33      0.50      0.40         2\n",
      "\n",
      "       accuracy                           0.73        11\n",
      "      macro avg       0.72      0.67      0.68        11\n",
      "   weighted avg       0.79      0.73      0.75        11\n",
      "\n",
      "\n",
      "dropout rate = 0.5\n",
      "\n",
      "Dropout Rate: 0.5\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Gentrifying       0.00      0.00      0.00         3\n",
      "  Higher-Income       0.62      0.83      0.71         6\n",
      "Non-Gentrifying       0.33      0.50      0.40         2\n",
      "\n",
      "       accuracy                           0.55        11\n",
      "      macro avg       0.32      0.44      0.37        11\n",
      "   weighted avg       0.40      0.55      0.46        11\n",
      "\n",
      "\n",
      "Dropout Rate exp results:\n",
      "Dropout Rate: 0.0, Test Accuracy: 0.73\n",
      "Dropout Rate: 0.1, Test Accuracy: 0.64\n",
      "Dropout Rate: 0.3, Test Accuracy: 0.73\n",
      "Dropout Rate: 0.5, Test Accuracy: 0.55\n"
     ]
    }
   ],
   "source": [
    "# increase class weighting to try to componsate for inbalanced data set\n",
    "class_weights = np.array([2.0, 1.0, 2.0])  # Gentrifying, Higher-Income, Non-Gentrifying\n",
    "\n",
    "def build_and_train_model(dropout_rate):\n",
    "    np.random.seed(42)\n",
    "    d = X_train.shape[1]\n",
    "    h1 = 16\n",
    "    h2 = 8\n",
    "    o = 3\n",
    "    \n",
    "    W1 = np.random.randn(d, h1) / np.sqrt(d)\n",
    "    b1 = np.zeros((h1, 1))\n",
    "    W2 = np.random.randn(h1, h2) / np.sqrt(h1)\n",
    "    b2 = np.zeros((h2, 1))\n",
    "    W3 = np.random.randn(h2, o) / np.sqrt(h2)\n",
    "    b3 = np.zeros((o, 1))\n",
    "    \n",
    "    eta = 0.01 # all learning rates preformed the same so pick .01 for now, will revisit at the end\n",
    "    epochs = 1000\n",
    "    n = X_train.shape[0]\n",
    "    batch_size = 8 \n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        indices = np.random.permutation(n)\n",
    "        X_train_shuffled = X_train[indices]\n",
    "        y_train_shuffled = y_train[indices]\n",
    "        \n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start + batch_size, n)\n",
    "            X_batch = X_train_shuffled[start:end]\n",
    "            y_batch = y_train_shuffled[start:end]\n",
    "            \n",
    "            dW1, db1 = np.zeros_like(W1), np.zeros_like(b1)\n",
    "            dW2, db2 = np.zeros_like(W2), np.zeros_like(b2)\n",
    "            dW3, db3 = np.zeros_like(W3), np.zeros_like(b3)\n",
    "            loss = 0\n",
    "            \n",
    "            for i in range(X_batch.shape[0]):\n",
    "                x = X_batch[i].reshape(d, 1)\n",
    "                y_true = y_batch[i].reshape(o, 1)\n",
    "                \n",
    "                z1 = np.dot(W1.T,x) + b1\n",
    "                h1 = relu(z1)\n",
    "                h1, mask1 = apply_dropout(h1, dropout_rate)\n",
    "    \n",
    "                z2 = np.dot(W2.T, h1) + b2\n",
    "                h2 = relu(z2)\n",
    "                h2, mask2 = apply_dropout(h2, dropout_rate)\n",
    "    \n",
    "                z3 = np.dot(W3.T,h2) + b3\n",
    "                y_pred = softmax(z3)\n",
    "                \n",
    "                loss += compute_loss(y_true, y_pred, class_weights)\n",
    "                \n",
    "                dz3 = y_pred - y_true\n",
    "                dW3 += np.dot(h2, dz3.T)\n",
    "                db3 += dz3\n",
    "                \n",
    "                dh2 = np.dot(W3 , dz3)\n",
    "                dh2 *= mask2 / (1-dropout_rate)\n",
    "                dz2 = dh2 * relu_deriv(z2)\n",
    "                dW2 +=  np.dot(h1,dz2.T)\n",
    "                db2 += dz2\n",
    "                \n",
    "                dh1 = np.dot(W2, dz2)\n",
    "                dh1 *= mask1 / (1-dropout_rate)\n",
    "                dz1 = dh1 * relu_deriv(z1)\n",
    "                dW1 += np.dot(x , dz1.T)\n",
    "                db1 += dz1\n",
    "            \n",
    "            dW1 /= X_batch.shape[0]\n",
    "            db1 /= X_batch.shape[0]\n",
    "            dW2 /= X_batch.shape[0]\n",
    "            db2 /= X_batch.shape[0]\n",
    "            dW3 /= X_batch.shape[0]\n",
    "            db3 /= X_batch.shape[0]\n",
    "            \n",
    "            W1 -= eta * dW1\n",
    "            b1 -= eta * db1\n",
    "            W2 -= eta * dW2\n",
    "            b2 -= eta * db2\n",
    "            W3 -= eta * dW3\n",
    "            b3 -= eta * db3\n",
    "        \n",
    "    y_pred_test = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        x = X_test[i].reshape(d, 1)\n",
    "        z1 = np.dot(W1.T,x) + b1\n",
    "        h1 = relu(z1)\n",
    "    \n",
    "        z2 = np.dot(W2.T, h1) + b2\n",
    "        h2 = relu(z2)\n",
    "    \n",
    "        z3 = np.dot(W3.T,h2) + b3\n",
    "        y_pred = softmax(z3)        \n",
    "        y_pred_test.append(np.argmax(y_pred))\n",
    "    y_pred_test = np.array(y_pred_test)\n",
    "    y_test_labels = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test_labels, y_pred_test)\n",
    "    conf_matrix = confusion_matrix(y_test_labels, y_pred_test)\n",
    "    class_report = classification_report(y_test_labels, y_pred_test, labels=[0, 1, 2], target_names=le.classes_, zero_division=0)\n",
    "    \n",
    "    test_data_copy = test_data.copy()  # Avoid overwriting\n",
    "    test_data_copy['predicted_label'] = le.inverse_transform(y_pred_test)\n",
    "    \n",
    "    print(f\"\\nDropout Rate: {dropout_rate}\")\n",
    "    print(\"Classification Report:\\n\", class_report)\n",
    "    \n",
    "    return accuracy, test_data_copy\n",
    "\n",
    "# Dropout rates to test\n",
    "dropout_rates = [0.0, 0.1, 0.3, 0.5]\n",
    "\n",
    "\n",
    "results = []\n",
    "test_data_all = []\n",
    "\n",
    "for dropout_rate in dropout_rates:\n",
    "    print(f\"\\ndropout rate = {dropout_rate}\")\n",
    "    accuracy, test_data_result = build_and_train_model(dropout_rate)\n",
    "    results.append((dropout_rate, accuracy))\n",
    "    test_data_all.append(test_data_result)\n",
    "\n",
    "print(\"\\nDropout Rate exp results:\")\n",
    "for dropout_rate, acc in results:\n",
    "    print(f\"Dropout Rate: {dropout_rate}, Test Accuracy: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d8a1d6-7e19-4a6e-8cd6-2d5b662e6428",
   "metadata": {},
   "source": [
    "# Experimening with Number of Neurons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5521819b-9cd7-495a-ab9b-be619b8a9902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiment with neurons: (8, 4)\n",
      "\n",
      "Neurons: (8, 4)\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Gentrifying       0.00      0.00      0.00         3\n",
      "  Higher-Income       0.50      0.83      0.62         6\n",
      "Non-Gentrifying       0.00      0.00      0.00         2\n",
      "\n",
      "       accuracy                           0.45        11\n",
      "      macro avg       0.17      0.28      0.21        11\n",
      "   weighted avg       0.27      0.45      0.34        11\n",
      "\n",
      "\n",
      "Running experiment with neurons: (16, 8)\n",
      "\n",
      "Neurons: (16, 8)\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Gentrifying       1.00      0.67      0.80         3\n",
      "  Higher-Income       0.83      0.83      0.83         6\n",
      "Non-Gentrifying       0.33      0.50      0.40         2\n",
      "\n",
      "       accuracy                           0.73        11\n",
      "      macro avg       0.72      0.67      0.68        11\n",
      "   weighted avg       0.79      0.73      0.75        11\n",
      "\n",
      "\n",
      "Running experiment with neurons: (32, 16)\n",
      "\n",
      "Neurons: (32, 16)\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Gentrifying       1.00      0.67      0.80         3\n",
      "  Higher-Income       0.71      0.83      0.77         6\n",
      "Non-Gentrifying       0.50      0.50      0.50         2\n",
      "\n",
      "       accuracy                           0.73        11\n",
      "      macro avg       0.74      0.67      0.69        11\n",
      "   weighted avg       0.75      0.73      0.73        11\n",
      "\n",
      "\n",
      "Running experiment with neurons: (64, 32)\n",
      "\n",
      "Neurons: (64, 32)\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Gentrifying       1.00      0.33      0.50         3\n",
      "  Higher-Income       0.62      0.83      0.71         6\n",
      "Non-Gentrifying       0.50      0.50      0.50         2\n",
      "\n",
      "       accuracy                           0.64        11\n",
      "      macro avg       0.71      0.56      0.57        11\n",
      "   weighted avg       0.70      0.64      0.62        11\n",
      "\n",
      "\n",
      "Neuron Configuration Exp Results:\n",
      "Neurons: (8, 4), Test Accuracy: 0.45\n",
      "Neurons: (16, 8), Test Accuracy: 0.73\n",
      "Neurons: (32, 16), Test Accuracy: 0.73\n",
      "Neurons: (64, 32), Test Accuracy: 0.64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_and_train_model(neurons):\n",
    "    np.random.seed(42)\n",
    "    h1, h2 = neurons \n",
    "    d = X_train.shape[1] \n",
    "    o = 3 \n",
    "    \n",
    "    W1 = np.random.randn(d, h1) / np.sqrt(d)\n",
    "    b1 = np.zeros((h1, 1))\n",
    "    W2 = np.random.randn(h1, h2) / np.sqrt(h1)\n",
    "    b2 = np.zeros((h2, 1))\n",
    "    W3 = np.random.randn(h2, o) / np.sqrt(h2)\n",
    "    b3 = np.zeros((o, 1))\n",
    "    \n",
    "    eta = 0.01 # from exp 1\n",
    "    dropout_rate = 0.3  # from exp 2, again all had similar but pick .3 for a more balanced appraoch to be safer\n",
    "    epochs = 1000\n",
    "    n = X_train.shape[0]\n",
    "    batch_size = 8\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        indices = np.random.permutation(n)\n",
    "        X_train_shuffled = X_train[indices]\n",
    "        y_train_shuffled = y_train[indices]\n",
    "        \n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start + batch_size, n)\n",
    "            X_batch = X_train_shuffled[start:end]\n",
    "            y_batch = y_train_shuffled[start:end]\n",
    "            \n",
    "            dW1, db1 = np.zeros_like(W1), np.zeros_like(b1)\n",
    "            dW2, db2 = np.zeros_like(W2), np.zeros_like(b2)\n",
    "            dW3, db3 = np.zeros_like(W3), np.zeros_like(b3)\n",
    "            loss = 0\n",
    "            \n",
    "            for i in range(X_batch.shape[0]):\n",
    "                x = X_batch[i].reshape(d, 1)\n",
    "                y_true = y_batch[i].reshape(o, 1)\n",
    "                \n",
    "                z1 = np.dot(W1.T,x) + b1\n",
    "                h1 = relu(z1)\n",
    "                h1, mask1 = apply_dropout(h1, dropout_rate)\n",
    "    \n",
    "                z2 = np.dot(W2.T, h1) + b2\n",
    "                h2 = relu(z2)\n",
    "                h2, mask2 = apply_dropout(h2, dropout_rate)\n",
    "    \n",
    "                z3 = np.dot(W3.T,h2) + b3\n",
    "                y_pred = softmax(z3)\n",
    "    \n",
    "                \n",
    "                loss += compute_loss(y_true, y_pred, class_weights)\n",
    "                \n",
    "                dz3 = y_pred - y_true\n",
    "                dW3 += np.dot(h2, dz3.T)\n",
    "                db3 += dz3\n",
    "                \n",
    "                dh2 = np.dot(W3, dz3)\n",
    "                dh2 *= mask2 / (1-dropout_rate)\n",
    "                dz2 = dh2 * relu_deriv(z2)\n",
    "                dW2 += np.dot(h1,dz2.T)\n",
    "                db2 += dz2\n",
    "                \n",
    "                dh1 = np.dot(W2,dz2)\n",
    "                dh1 *= mask1 / (1-dropout_rate)\n",
    "                dz1 = dh1 * relu_deriv(z1)\n",
    "                dW1 += np.dot(x ,dz1.T)\n",
    "                db1 += dz1\n",
    "            \n",
    "            dW1 /= X_batch.shape[0]\n",
    "            db1 /= X_batch.shape[0]\n",
    "            dW2 /= X_batch.shape[0]\n",
    "            db2 /= X_batch.shape[0]\n",
    "            dW3 /= X_batch.shape[0]\n",
    "            db3 /= X_batch.shape[0]\n",
    "            \n",
    "            W1 -= eta * dW1\n",
    "            b1 -= eta * db1\n",
    "            W2 -= eta * dW2\n",
    "            b2 -= eta * db2\n",
    "            W3 -= eta * dW3\n",
    "            b3 -= eta * db3\n",
    "        \n",
    "    y_pred_test = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        x = X_test[i].reshape(d, 1)\n",
    "        z1 = np.dot(W1.T, x) + b1\n",
    "        h1 = relu(z1)\n",
    "            \n",
    "        z2 = np.dot(W2.T, h1) + b2\n",
    "        h2 = relu(z2)\n",
    "    \n",
    "        z3 = np.dot(W3.T, h2) + b3\n",
    "        y_pred = softmax(z3)\n",
    "        y_pred_test.append(np.argmax(y_pred))\n",
    "    y_pred_test = np.array(y_pred_test)\n",
    "    y_test_labels = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test_labels, y_pred_test)\n",
    "    conf_matrix = confusion_matrix(y_test_labels, y_pred_test)\n",
    "    class_report = classification_report(y_test_labels, y_pred_test, labels=[0, 1, 2], target_names=le.classes_, zero_division=0)\n",
    "    \n",
    "    test_data_copy = test_data.copy()  # Avoid overwriting\n",
    "    test_data_copy['predicted_label'] = le.inverse_transform(y_pred_test)\n",
    "    \n",
    "    print(f\"\\nNeurons: {neurons}\")\n",
    "    print(\"Classification Report:\\n\", class_report)\n",
    "    \n",
    "    return accuracy, test_data_copy\n",
    "\n",
    "neuron_configs = [(8, 4), (16, 8), (32, 16), (64, 32)]\n",
    "results = []\n",
    "test_data_all = []\n",
    "\n",
    "\n",
    "for neurons in neuron_configs:\n",
    "    print(f\"\\nRunning experiment with neurons: {neurons}\")\n",
    "    accuracy, test_data_result = build_and_train_model(neurons)\n",
    "    results.append((neurons, accuracy))\n",
    "    test_data_all.append(test_data_result)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nNeuron Configuration Exp Results:\")\n",
    "for neurons, acc in results:\n",
    "    print(f\"Neurons: {neurons}, Test Accuracy: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5206ed77-ff67-4b72-bec4-380b78003b17",
   "metadata": {},
   "source": [
    "# Experimening Batch Size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1920932-528d-426c-8069-fbc7c497bd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "batch size: 4\n",
      "\n",
      "Batch Size: 4\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Gentrifying       1.00      0.67      0.80         3\n",
      "  Higher-Income       0.71      0.83      0.77         6\n",
      "Non-Gentrifying       0.50      0.50      0.50         2\n",
      "\n",
      "       accuracy                           0.73        11\n",
      "      macro avg       0.74      0.67      0.69        11\n",
      "   weighted avg       0.75      0.73      0.73        11\n",
      "\n",
      "\n",
      "batch size: 8\n",
      "\n",
      "Batch Size: 8\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Gentrifying       1.00      0.67      0.80         3\n",
      "  Higher-Income       0.71      0.83      0.77         6\n",
      "Non-Gentrifying       0.50      0.50      0.50         2\n",
      "\n",
      "       accuracy                           0.73        11\n",
      "      macro avg       0.74      0.67      0.69        11\n",
      "   weighted avg       0.75      0.73      0.73        11\n",
      "\n",
      "\n",
      "batch size: 16\n",
      "\n",
      "Batch Size: 16\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Gentrifying       0.50      0.33      0.40         3\n",
      "  Higher-Income       0.62      0.83      0.71         6\n",
      "Non-Gentrifying       0.00      0.00      0.00         2\n",
      "\n",
      "       accuracy                           0.55        11\n",
      "      macro avg       0.38      0.39      0.37        11\n",
      "   weighted avg       0.48      0.55      0.50        11\n",
      "\n",
      "\n",
      "batch size: 32\n",
      "\n",
      "Batch Size: 32\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Gentrifying       0.50      0.33      0.40         3\n",
      "  Higher-Income       0.56      0.83      0.67         6\n",
      "Non-Gentrifying       0.00      0.00      0.00         2\n",
      "\n",
      "       accuracy                           0.55        11\n",
      "      macro avg       0.35      0.39      0.36        11\n",
      "   weighted avg       0.44      0.55      0.47        11\n",
      "\n",
      "\n",
      "Summary of Batch Size Experiment:\n",
      "Batch Size: 4, Test Accuracy: 0.73\n",
      "Batch Size: 8, Test Accuracy: 0.73\n",
      "Batch Size: 16, Test Accuracy: 0.55\n",
      "Batch Size: 32, Test Accuracy: 0.55\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class_weights = np.array([2.5, 1.0, 2.5])  # Adjusted to boost minority classes\n",
    "\n",
    "\n",
    "def build_and_train_model(batch_size):\n",
    "    np.random.seed(42)\n",
    "    d = X_train.shape[1] \n",
    "    h1 = 32 # picked 31 and 16  even tho 18,8 has similar result because its a complex classification prob, lean towards more complex model \n",
    "    h2 = 16\n",
    "    o = 3 \n",
    "    \n",
    "    W1 = np.random.randn(d, h1) / np.sqrt(d)\n",
    "    b1 = np.zeros((h1, 1))\n",
    "    W2 = np.random.randn(h1, h2) / np.sqrt(h1)\n",
    "    b2 = np.zeros((h2, 1))\n",
    "    W3 = np.random.randn(h2, o) / np.sqrt(h2)\n",
    "    b3 = np.zeros((o, 1))\n",
    "    \n",
    "    eta = 0.01  # from exp 1\n",
    "    dropout_rate = 0.3  # from exp 2\n",
    "    epochs = 1000\n",
    "    n = X_train.shape[0]\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        indices = np.random.permutation(n)\n",
    "        X_train_shuffled = X_train[indices]\n",
    "        y_train_shuffled = y_train[indices]\n",
    "        \n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start + batch_size, n)\n",
    "            X_batch = X_train_shuffled[start:end]\n",
    "            y_batch = y_train_shuffled[start:end]\n",
    "            \n",
    "            dW1, db1 = np.zeros_like(W1), np.zeros_like(b1)\n",
    "            dW2, db2 = np.zeros_like(W2), np.zeros_like(b2)\n",
    "            dW3, db3 = np.zeros_like(W3), np.zeros_like(b3)\n",
    "            loss = 0\n",
    "            \n",
    "            for i in range(X_batch.shape[0]):\n",
    "                x = X_batch[i].reshape(d, 1)\n",
    "                y_true = y_batch[i].reshape(o, 1)\n",
    "                \n",
    "                z1 = np.dot(W1.T,x) + b1\n",
    "                h1 = relu(z1)\n",
    "                h1, mask1 = apply_dropout(h1, dropout_rate)\n",
    "    \n",
    "                z2 = np.dot(W2.T, h1) + b2\n",
    "                h2 = relu(z2)\n",
    "                h2, mask2 = apply_dropout(h2, dropout_rate)\n",
    "    \n",
    "                z3 = np.dot(W3.T,h2) + b3\n",
    "                y_pred = softmax(z3)\n",
    "                \n",
    "                loss += compute_loss(y_true, y_pred, class_weights)\n",
    "                \n",
    "                dz3 = y_pred - y_true\n",
    "                dW3 += np.dot(h2,dz3.T)\n",
    "                db3 += dz3\n",
    "                \n",
    "                dh2 = np.dot(W3,dz3)\n",
    "                dh2 *= mask2 / (1-dropout_rate)\n",
    "                dz2 = dh2 * relu_deriv(z2)\n",
    "                dW2 += np.dot(h1,dz2.T)\n",
    "                db2 += dz2\n",
    "                \n",
    "                dh1 = np.dot(W2, dz2)\n",
    "                dh1 *= mask1 / (1-dropout_rate)\n",
    "                dz1 = dh1 * relu_deriv(z1)\n",
    "                dW1 += np.dot(x,dz1.T)\n",
    "                db1 += dz1\n",
    "            \n",
    "            dW1 /= X_batch.shape[0]\n",
    "            db1 /= X_batch.shape[0]\n",
    "            dW2 /= X_batch.shape[0]\n",
    "            db2 /= X_batch.shape[0]\n",
    "            dW3 /= X_batch.shape[0]\n",
    "            db3 /= X_batch.shape[0]\n",
    "            \n",
    "            W1 -= eta * dW1\n",
    "            b1 -= eta * db1\n",
    "            W2 -= eta * dW2\n",
    "            b2 -= eta * db2\n",
    "            W3 -= eta * dW3\n",
    "            b3 -= eta * db3\n",
    "\n",
    "    \n",
    "    y_pred_test = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        x = X_test[i].reshape(d, 1)\n",
    "        \n",
    "        z1 = np.dot(W1.T,x) + b1\n",
    "        h1 = relu(z1)\n",
    "        z2 = np.dot(W2.T, h1) + b2\n",
    "        h2 = relu(z2)\n",
    "        z3 = np.dot(W3.T,h2) + b3\n",
    "        y_pred = softmax(z3)\n",
    "        \n",
    "        y_pred_test.append(np.argmax(y_pred))\n",
    "    y_pred_test = np.array(y_pred_test)\n",
    "    y_test_labels = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test_labels, y_pred_test)\n",
    "    conf_matrix = confusion_matrix(y_test_labels, y_pred_test)\n",
    "    class_report = classification_report(y_test_labels, y_pred_test, labels=[0, 1, 2], target_names=le.classes_, zero_division=0)\n",
    "    \n",
    "    test_data_copy = test_data.copy()\n",
    "    test_data_copy['predicted_label'] = le.inverse_transform(y_pred_test)\n",
    "    \n",
    "    print(f\"\\nBatch Size: {batch_size}\")\n",
    "    print(\"Classification Report:\\n\", class_report)\n",
    "    \n",
    "    return accuracy, test_data_copy\n",
    "\n",
    "batch_sizes = [4, 8, 16, 32]\n",
    "\n",
    "results = []\n",
    "val_histories = []\n",
    "test_data_all = []\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"\\nbatch size: {batch_size}\")\n",
    "    accuracy, test_data_result = build_and_train_model(batch_size)\n",
    "    results.append((batch_size, accuracy))\n",
    "    test_data_all.append(test_data_result)\n",
    "\n",
    "\n",
    "print(\"\\nSummary of Batch Size Experiment:\")\n",
    "for batch_size, acc in results:\n",
    "    print(f\"Batch Size: {batch_size}, Test Accuracy: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c4d38a-1b84-4bdc-bf3e-d3ab6e4055af",
   "metadata": {},
   "source": [
    "# Learning Rate Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c68be51-eaaf-4e41-afa8-7d10a9b0d92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate: 0.001\n",
      "\n",
      "Learning Rate: 0.001\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Gentrifying       0.00      0.00      0.00         3\n",
      "  Higher-Income       0.55      1.00      0.71         6\n",
      "Non-Gentrifying       0.00      0.00      0.00         2\n",
      "\n",
      "       accuracy                           0.55        11\n",
      "      macro avg       0.18      0.33      0.24        11\n",
      "   weighted avg       0.30      0.55      0.39        11\n",
      "\n",
      "\n",
      "learning rate: 0.005\n",
      "\n",
      "Learning Rate: 0.005\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Gentrifying       0.50      0.33      0.40         3\n",
      "  Higher-Income       0.62      0.83      0.71         6\n",
      "Non-Gentrifying       0.00      0.00      0.00         2\n",
      "\n",
      "       accuracy                           0.55        11\n",
      "      macro avg       0.38      0.39      0.37        11\n",
      "   weighted avg       0.48      0.55      0.50        11\n",
      "\n",
      "\n",
      "learning rate: 0.01\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Gentrifying       1.00      0.67      0.80         3\n",
      "  Higher-Income       0.71      0.83      0.77         6\n",
      "Non-Gentrifying       0.50      0.50      0.50         2\n",
      "\n",
      "       accuracy                           0.73        11\n",
      "      macro avg       0.74      0.67      0.69        11\n",
      "   weighted avg       0.75      0.73      0.73        11\n",
      "\n",
      "\n",
      "learning rate: 0.05\n",
      "\n",
      "Learning Rate: 0.05\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Gentrifying       1.00      0.67      0.80         3\n",
      "  Higher-Income       0.71      0.83      0.77         6\n",
      "Non-Gentrifying       0.50      0.50      0.50         2\n",
      "\n",
      "       accuracy                           0.73        11\n",
      "      macro avg       0.74      0.67      0.69        11\n",
      "   weighted avg       0.75      0.73      0.73        11\n",
      "\n",
      "\n",
      "Summar of Learning Rate Experiment 2:\n",
      "Learning Rate: 0.001, Test Accuracy: 0.55\n",
      "Learning Rate: 0.005, Test Accuracy: 0.55\n",
      "Learning Rate: 0.01, Test Accuracy: 0.73\n",
      "Learning Rate: 0.05, Test Accuracy: 0.73\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_and_train_model(eta):\n",
    "    np.random.seed(42)\n",
    "    d = X_train.shape[1]\n",
    "    h1 = 32\n",
    "    h2 = 16\n",
    "    o = 3\n",
    "    \n",
    "    W1 = np.random.randn(d, h1) / np.sqrt(d)\n",
    "    b1 = np.zeros((h1, 1))\n",
    "    W2 = np.random.randn(h1, h2) / np.sqrt(h1)\n",
    "    b2 = np.zeros((h2, 1))\n",
    "    W3 = np.random.randn(h2, o) / np.sqrt(h2)\n",
    "    b3 = np.zeros((o, 1))\n",
    "    \n",
    "    dropout_rate = 0.3\n",
    "    batch_size = 8 # picked this one from the batch experiment bc the confusion matrix is best balanced\n",
    "    epochs = 1000\n",
    "    n = X_train.shape[0]\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        indices = np.random.permutation(n)\n",
    "        X_train_shuffled = X_train[indices]\n",
    "        y_train_shuffled = y_train[indices]\n",
    "        \n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start + batch_size, n)\n",
    "            X_batch = X_train_shuffled[start:end]\n",
    "            y_batch = y_train_shuffled[start:end]\n",
    "            \n",
    "            dW1, db1 = np.zeros_like(W1), np.zeros_like(b1)\n",
    "            dW2, db2 = np.zeros_like(W2), np.zeros_like(b2)\n",
    "            dW3, db3 = np.zeros_like(W3), np.zeros_like(b3)\n",
    "            loss = 0\n",
    "            \n",
    "            for i in range(X_batch.shape[0]):\n",
    "                x = X_batch[i].reshape(d, 1)\n",
    "                y_true = y_batch[i].reshape(o, 1)\n",
    "                \n",
    "                z1 = np.dot(W1.T,x) + b1\n",
    "                h1 = relu(z1)\n",
    "                h1, mask1 = apply_dropout(h1, dropout_rate)\n",
    "    \n",
    "                z2 = np.dot(W2.T, h1) + b2\n",
    "                h2 = relu(z2)\n",
    "                h2, mask2 = apply_dropout(h2, dropout_rate)\n",
    "    \n",
    "                z3 = np.dot(W3.T,h2) + b3\n",
    "                y_pred = softmax(z3)\n",
    "                \n",
    "                loss += compute_loss(y_true, y_pred, class_weights)\n",
    "                \n",
    "                dz3 = y_pred - y_true\n",
    "                dW3 += np.dot(h2, dz3.T)\n",
    "                db3 += dz3\n",
    "                \n",
    "                dh2 = np.dot(W3, dz3)\n",
    "                dh2 *= mask2 / (1-dropout_rate)\n",
    "                dz2 = dh2 * relu_deriv(z2)\n",
    "                dW2 += np.dot(h1, dz2.T)\n",
    "                db2 += dz2\n",
    "                \n",
    "                dh1 = np.dot(W2,dz2)\n",
    "                dh1 *= mask1 / (1-dropout_rate)\n",
    "                dz1 = dh1 * relu_deriv(z1)\n",
    "                dW1 += np.dot(x,dz1.T)\n",
    "                db1 += dz1\n",
    "            \n",
    "            dW1 /= X_batch.shape[0]\n",
    "            db1 /= X_batch.shape[0]\n",
    "            dW2 /= X_batch.shape[0]\n",
    "            db2 /= X_batch.shape[0]\n",
    "            dW3 /= X_batch.shape[0]\n",
    "            db3 /= X_batch.shape[0]\n",
    "            \n",
    "            W1 -= eta * dW1\n",
    "            b1 -= eta * db1\n",
    "            W2 -= eta * dW2\n",
    "            b2 -= eta * db2\n",
    "            W3 -= eta * dW3\n",
    "            b3 -= eta * db3\n",
    "\n",
    "\n",
    "\n",
    "    y_pred_test = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        x = X_test[i].reshape(d, 1)\n",
    "        z1 = np.dot(W1.T,x) + b1\n",
    "        h1 = relu(z1)\n",
    "        z2 = np.dot(W2.T, h1) + b2\n",
    "        h2 = relu(z2)\n",
    "        z3 = np.dot(W3.T,h2) + b3\n",
    "        y_pred = softmax(z3)\n",
    "        \n",
    "        y_pred_test.append(np.argmax(y_pred))\n",
    "    y_pred_test = np.array(y_pred_test)\n",
    "    y_test_labels = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test_labels, y_pred_test)\n",
    "    conf_matrix = confusion_matrix(y_test_labels, y_pred_test)\n",
    "    class_report = classification_report(y_test_labels, y_pred_test, labels=[0, 1, 2], target_names=le.classes_, zero_division=0)\n",
    "    \n",
    "    test_data_copy = test_data.copy()\n",
    "    test_data_copy['predicted_label'] = le.inverse_transform(y_pred_test)\n",
    "    \n",
    "    print(f\"\\nLearning Rate: {eta}\")\n",
    "    print(\"Classification Report:\\n\", class_report)\n",
    "    \n",
    "    return accuracy, test_data_copy\n",
    "\n",
    "learning_rates = [0.001, 0.005, 0.01, 0.05]\n",
    "\n",
    "results = []\n",
    "val_histories = []\n",
    "test_data_all = []\n",
    "\n",
    "for eta in learning_rates:\n",
    "    print(f\"\\nlearning rate: {eta}\")\n",
    "    accuracy, test_data_result = build_and_train_model(eta)\n",
    "    results.append((eta, accuracy))\n",
    "    test_data_all.append(test_data_result)\n",
    "\n",
    "\n",
    "print(\"\\nSummar of Learning Rate Experiment 2:\")\n",
    "for eta, acc in results:\n",
    "    print(f\"Learning Rate: {eta}, Test Accuracy: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a75b17-7e95-4cdc-947a-9d73391bc0fd",
   "metadata": {},
   "source": [
    "## Experiment with number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ee68e28-6a47-446a-87f3-52fb71269796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiment with epoch of: 400\n",
      "\n",
      " Epochs trained with: 400\n",
      "Test Set Classifications:\n",
      "                 Sub-Borough Area     actual_label  predicted_label\n",
      "44            Flushing/Whitestone      Gentrifying    Higher-Income\n",
      "45        Hillcrest/Fresh Meadows    Higher-Income  Non-Gentrifying\n",
      "46           Ozone Park/Woodhaven    Higher-Income    Higher-Income\n",
      "47  South Ozone Park/Howard Beach  Non-Gentrifying    Higher-Income\n",
      "48            Bayside/Little Neck    Higher-Income    Higher-Income\n",
      "49                        Jamaica      Gentrifying    Higher-Income\n",
      "50                 Queens Village    Higher-Income    Higher-Income\n",
      "51                      Rockaways    Higher-Income    Higher-Income\n",
      "52                    North Shore  Non-Gentrifying    Higher-Income\n",
      "53                     Mid-Island      Gentrifying      Gentrifying\n",
      "54                    South Shore    Higher-Income    Higher-Income\n",
      "Test Accuracy: 0.55\n",
      "Confusion Matrix:\n",
      " [[1 2 0]\n",
      " [0 5 1]\n",
      " [0 2 0]]\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Gentrifying       1.00      0.33      0.50         3\n",
      "  Higher-Income       0.56      0.83      0.67         6\n",
      "Non-Gentrifying       0.00      0.00      0.00         2\n",
      "\n",
      "       accuracy                           0.55        11\n",
      "      macro avg       0.52      0.39      0.39        11\n",
      "   weighted avg       0.58      0.55      0.50        11\n",
      "\n",
      "\n",
      "Running experiment with epoch of: 600\n",
      "\n",
      " Epochs trained with: 600\n",
      "Test Set Classifications:\n",
      "                 Sub-Borough Area     actual_label  predicted_label\n",
      "44            Flushing/Whitestone      Gentrifying    Higher-Income\n",
      "45        Hillcrest/Fresh Meadows    Higher-Income  Non-Gentrifying\n",
      "46           Ozone Park/Woodhaven    Higher-Income    Higher-Income\n",
      "47  South Ozone Park/Howard Beach  Non-Gentrifying      Gentrifying\n",
      "48            Bayside/Little Neck    Higher-Income    Higher-Income\n",
      "49                        Jamaica      Gentrifying    Higher-Income\n",
      "50                 Queens Village    Higher-Income    Higher-Income\n",
      "51                      Rockaways    Higher-Income    Higher-Income\n",
      "52                    North Shore  Non-Gentrifying    Higher-Income\n",
      "53                     Mid-Island      Gentrifying      Gentrifying\n",
      "54                    South Shore    Higher-Income    Higher-Income\n",
      "Test Accuracy: 0.55\n",
      "Confusion Matrix:\n",
      " [[1 2 0]\n",
      " [0 5 1]\n",
      " [1 1 0]]\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Gentrifying       0.50      0.33      0.40         3\n",
      "  Higher-Income       0.62      0.83      0.71         6\n",
      "Non-Gentrifying       0.00      0.00      0.00         2\n",
      "\n",
      "       accuracy                           0.55        11\n",
      "      macro avg       0.38      0.39      0.37        11\n",
      "   weighted avg       0.48      0.55      0.50        11\n",
      "\n",
      "\n",
      "Running experiment with epoch of: 800\n",
      "\n",
      " Epochs trained with: 800\n",
      "Test Set Classifications:\n",
      "                 Sub-Borough Area     actual_label  predicted_label\n",
      "44            Flushing/Whitestone      Gentrifying    Higher-Income\n",
      "45        Hillcrest/Fresh Meadows    Higher-Income  Non-Gentrifying\n",
      "46           Ozone Park/Woodhaven    Higher-Income    Higher-Income\n",
      "47  South Ozone Park/Howard Beach  Non-Gentrifying  Non-Gentrifying\n",
      "48            Bayside/Little Neck    Higher-Income    Higher-Income\n",
      "49                        Jamaica      Gentrifying    Higher-Income\n",
      "50                 Queens Village    Higher-Income    Higher-Income\n",
      "51                      Rockaways    Higher-Income    Higher-Income\n",
      "52                    North Shore  Non-Gentrifying    Higher-Income\n",
      "53                     Mid-Island      Gentrifying      Gentrifying\n",
      "54                    South Shore    Higher-Income    Higher-Income\n",
      "Test Accuracy: 0.64\n",
      "Confusion Matrix:\n",
      " [[1 2 0]\n",
      " [0 5 1]\n",
      " [0 1 1]]\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Gentrifying       1.00      0.33      0.50         3\n",
      "  Higher-Income       0.62      0.83      0.71         6\n",
      "Non-Gentrifying       0.50      0.50      0.50         2\n",
      "\n",
      "       accuracy                           0.64        11\n",
      "      macro avg       0.71      0.56      0.57        11\n",
      "   weighted avg       0.70      0.64      0.62        11\n",
      "\n",
      "\n",
      "Running experiment with epoch of: 1000\n",
      "\n",
      " Epochs trained with: 1000\n",
      "Test Set Classifications:\n",
      "                 Sub-Borough Area     actual_label  predicted_label\n",
      "44            Flushing/Whitestone      Gentrifying      Gentrifying\n",
      "45        Hillcrest/Fresh Meadows    Higher-Income  Non-Gentrifying\n",
      "46           Ozone Park/Woodhaven    Higher-Income    Higher-Income\n",
      "47  South Ozone Park/Howard Beach  Non-Gentrifying  Non-Gentrifying\n",
      "48            Bayside/Little Neck    Higher-Income    Higher-Income\n",
      "49                        Jamaica      Gentrifying    Higher-Income\n",
      "50                 Queens Village    Higher-Income    Higher-Income\n",
      "51                      Rockaways    Higher-Income    Higher-Income\n",
      "52                    North Shore  Non-Gentrifying    Higher-Income\n",
      "53                     Mid-Island      Gentrifying      Gentrifying\n",
      "54                    South Shore    Higher-Income    Higher-Income\n",
      "Test Accuracy: 0.73\n",
      "Confusion Matrix:\n",
      " [[2 1 0]\n",
      " [0 5 1]\n",
      " [0 1 1]]\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Gentrifying       1.00      0.67      0.80         3\n",
      "  Higher-Income       0.71      0.83      0.77         6\n",
      "Non-Gentrifying       0.50      0.50      0.50         2\n",
      "\n",
      "       accuracy                           0.73        11\n",
      "      macro avg       0.74      0.67      0.69        11\n",
      "   weighted avg       0.75      0.73      0.73        11\n",
      "\n",
      "\n",
      "Running experiment with epoch of: 1500\n",
      "\n",
      " Epochs trained with: 1500\n",
      "Test Set Classifications:\n",
      "                 Sub-Borough Area     actual_label  predicted_label\n",
      "44            Flushing/Whitestone      Gentrifying      Gentrifying\n",
      "45        Hillcrest/Fresh Meadows    Higher-Income  Non-Gentrifying\n",
      "46           Ozone Park/Woodhaven    Higher-Income    Higher-Income\n",
      "47  South Ozone Park/Howard Beach  Non-Gentrifying  Non-Gentrifying\n",
      "48            Bayside/Little Neck    Higher-Income    Higher-Income\n",
      "49                        Jamaica      Gentrifying    Higher-Income\n",
      "50                 Queens Village    Higher-Income    Higher-Income\n",
      "51                      Rockaways    Higher-Income    Higher-Income\n",
      "52                    North Shore  Non-Gentrifying    Higher-Income\n",
      "53                     Mid-Island      Gentrifying      Gentrifying\n",
      "54                    South Shore    Higher-Income    Higher-Income\n",
      "Test Accuracy: 0.73\n",
      "Confusion Matrix:\n",
      " [[2 1 0]\n",
      " [0 5 1]\n",
      " [0 1 1]]\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Gentrifying       1.00      0.67      0.80         3\n",
      "  Higher-Income       0.71      0.83      0.77         6\n",
      "Non-Gentrifying       0.50      0.50      0.50         2\n",
      "\n",
      "       accuracy                           0.73        11\n",
      "      macro avg       0.74      0.67      0.69        11\n",
      "   weighted avg       0.75      0.73      0.73        11\n",
      "\n",
      "\n",
      "Running experiment with epoch of: 2000\n",
      "\n",
      " Epochs trained with: 2000\n",
      "Test Set Classifications:\n",
      "                 Sub-Borough Area     actual_label  predicted_label\n",
      "44            Flushing/Whitestone      Gentrifying      Gentrifying\n",
      "45        Hillcrest/Fresh Meadows    Higher-Income  Non-Gentrifying\n",
      "46           Ozone Park/Woodhaven    Higher-Income    Higher-Income\n",
      "47  South Ozone Park/Howard Beach  Non-Gentrifying  Non-Gentrifying\n",
      "48            Bayside/Little Neck    Higher-Income    Higher-Income\n",
      "49                        Jamaica      Gentrifying    Higher-Income\n",
      "50                 Queens Village    Higher-Income    Higher-Income\n",
      "51                      Rockaways    Higher-Income    Higher-Income\n",
      "52                    North Shore  Non-Gentrifying    Higher-Income\n",
      "53                     Mid-Island      Gentrifying      Gentrifying\n",
      "54                    South Shore    Higher-Income    Higher-Income\n",
      "Test Accuracy: 0.73\n",
      "Confusion Matrix:\n",
      " [[2 1 0]\n",
      " [0 5 1]\n",
      " [0 1 1]]\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Gentrifying       1.00      0.67      0.80         3\n",
      "  Higher-Income       0.71      0.83      0.77         6\n",
      "Non-Gentrifying       0.50      0.50      0.50         2\n",
      "\n",
      "       accuracy                           0.73        11\n",
      "      macro avg       0.74      0.67      0.69        11\n",
      "   weighted avg       0.75      0.73      0.73        11\n",
      "\n",
      "\n",
      "Summary of Epcoh Experiment:\n",
      "Epoch 400, Test Accuracy: 0.55\n",
      "Epoch 600, Test Accuracy: 0.55\n",
      "Epoch 800, Test Accuracy: 0.64\n",
      "Epoch 1000, Test Accuracy: 0.73\n",
      "Epoch 1500, Test Accuracy: 0.73\n",
      "Epoch 2000, Test Accuracy: 0.73\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_and_train_model(epochs_amt):\n",
    "    np.random.seed(42)\n",
    "    d = X_train.shape[1]\n",
    "    h1 = 32 \n",
    "    h2 = 16\n",
    "    o = 3\n",
    "    \n",
    "    W1 = np.random.randn(d, h1) / np.sqrt(d)\n",
    "    b1 = np.zeros((h1, 1))\n",
    "    W2 = np.random.randn(h1, h2) / np.sqrt(h1)\n",
    "    b2 = np.zeros((h2, 1))\n",
    "    W3 = np.random.randn(h2, o) / np.sqrt(h2)\n",
    "    b3 = np.zeros((o, 1))\n",
    "    \n",
    "    dropout_rate = 0.3\n",
    "    batch_size = 8 # picked this one from the batch experiment bc the confusion matrix is best balanced\n",
    "    eta = .01 # best from second learning rate exp, tied with .05 go with .1\n",
    "    n = X_train.shape[0]\n",
    "    \n",
    "    for epoch in range(epochs_amt):\n",
    "        indices = np.random.permutation(n)\n",
    "        X_train_shuffled = X_train[indices]\n",
    "        y_train_shuffled = y_train[indices]\n",
    "        \n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start + batch_size, n)\n",
    "            X_batch = X_train_shuffled[start:end]\n",
    "            y_batch = y_train_shuffled[start:end]\n",
    "            \n",
    "            dW1, db1 = np.zeros_like(W1), np.zeros_like(b1)\n",
    "            dW2, db2 = np.zeros_like(W2), np.zeros_like(b2)\n",
    "            dW3, db3 = np.zeros_like(W3), np.zeros_like(b3)\n",
    "            loss = 0\n",
    "            \n",
    "            for i in range(X_batch.shape[0]):\n",
    "                x = X_batch[i].reshape(d, 1)\n",
    "                y_true = y_batch[i].reshape(o, 1)\n",
    "                \n",
    "                z1 = np.dot(W1.T,x) + b1\n",
    "                h1 = relu(z1)\n",
    "                h1, mask1 = apply_dropout(h1, dropout_rate)\n",
    "    \n",
    "                z2 = np.dot(W2.T, h1) + b2\n",
    "                h2 = relu(z2)\n",
    "                h2, mask2 = apply_dropout(h2, dropout_rate)\n",
    "    \n",
    "                z3 = np.dot(W3.T,h2) + b3\n",
    "                y_pred = softmax(z3)\n",
    "                \n",
    "                loss += compute_loss(y_true, y_pred, class_weights)\n",
    "                \n",
    "                dz3 = y_pred - y_true\n",
    "                dW3 += np.dot(h2, dz3.T)\n",
    "                db3 += dz3\n",
    "                \n",
    "                dh2 = np.dot(W3, dz3)\n",
    "                dh2 *= mask2 / (1-dropout_rate)\n",
    "                dz2 = dh2 * relu_deriv(z2)\n",
    "                dW2 += np.dot(h1,dz2.T)\n",
    "                db2 += dz2\n",
    "                \n",
    "                dh1 = np.dot(W2,dz2)\n",
    "                dh1 *= mask1 / (1-dropout_rate)\n",
    "                dz1 = dh1 * relu_deriv(z1)\n",
    "                dW1 += np.dot(x,dz1.T)\n",
    "                db1 += dz1\n",
    "            \n",
    "            dW1 /= X_batch.shape[0]\n",
    "            db1 /= X_batch.shape[0]\n",
    "            dW2 /= X_batch.shape[0]\n",
    "            db2 /= X_batch.shape[0]\n",
    "            dW3 /= X_batch.shape[0]\n",
    "            db3 /= X_batch.shape[0]\n",
    "            \n",
    "            W1 -= eta * dW1\n",
    "            b1 -= eta * db1\n",
    "            W2 -= eta * dW2\n",
    "            b2 -= eta * db2\n",
    "            W3 -= eta * dW3\n",
    "            b3 -= eta * db3\n",
    "    \n",
    "\n",
    "    y_pred_test = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        x = X_test[i].reshape(d, 1)\n",
    "        z1 = np.dot(W1.T,x) + b1\n",
    "        h1 = relu(z1)\n",
    "        z2 = np.dot(W2.T, h1) + b2\n",
    "        h2 = relu(z2)\n",
    "        z3 = np.dot(W3.T,h2) + b3\n",
    "        y_pred = softmax(z3)\n",
    "        \n",
    "        y_pred_test.append(np.argmax(y_pred))\n",
    "    y_pred_test = np.array(y_pred_test)\n",
    "    y_test_labels = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test_labels, y_pred_test)\n",
    "    conf_matrix = confusion_matrix(y_test_labels, y_pred_test)\n",
    "    class_report = classification_report(y_test_labels, y_pred_test, labels=[0, 1, 2], target_names=le.classes_, zero_division=0)\n",
    "    \n",
    "    test_data_copy = test_data.copy()\n",
    "    test_data_copy['predicted_label'] = le.inverse_transform(y_pred_test)\n",
    "    \n",
    "    print(f\"\\n Epochs trained with: {epochs_amt}\")\n",
    "    print(\"Test Set Classifications:\")\n",
    "    print(test_data_copy[['Sub-Borough Area', 'actual_label', 'predicted_label']])\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}\")\n",
    "    print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "    print(\"Classification Report:\\n\", class_report)\n",
    "    \n",
    "    return accuracy, test_data_copy\n",
    "\n",
    "epoch_range = [ 400,600, 800, 1000, 1500, 2000]\n",
    "\n",
    "results = []\n",
    "test_data_all = []\n",
    "\n",
    "for epoch in epoch_range:\n",
    "    print(f\"\\nRunning experiment with epoch of: {epoch}\")\n",
    "    accuracy, test_data_result = build_and_train_model(epoch)\n",
    "    results.append((epoch, accuracy))\n",
    "    test_data_all.append(test_data_result)\n",
    "\n",
    "\n",
    "print(\"\\nSummary of Epcoh Experiment:\")\n",
    "for epoch, acc in results:\n",
    "    print(f\"Epoch {epoch}, Test Accuracy: {acc:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
